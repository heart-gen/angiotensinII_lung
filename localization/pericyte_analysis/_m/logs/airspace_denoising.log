2025-12-04 18:16:08 - **** Job starts ****
2025-12-04 18:16:08 - **** Bridges-2 info ****
User: kbenjamin
Job id: 36419173
Job name: airspace_denoising
Node name: 
Hostname: r042
Task id: N/A

Currently Loaded Modules:
  1) anaconda3/2024.10-1   2) gcc/13.3.1-p20240614

 

2025-12-04 18:16:09 - **** Loading mamba environment ****
2025-12-04 18:16:09 - **** Run analysis ****
2025-12-04 18:19:20,746 [INFO] Define masks
2025-12-04 18:19:20,861 [INFO] Subset data for training: perivascular
/ocean/projects/bio250020p/shared/opt/env/scRNA_env/lib/python3.10/site-packages/scanpy/preprocessing/_highly_variable_genes.py:693: UserWarning: `n_top_genes` > number of normalized dispersions, returning all genes with normalized dispersions.
  df = _highly_variable_genes_batched(
2025-12-04 18:19:40,994 [INFO] Training scVI model on perivascular cells
/ocean/projects/bio250020p/shared/opt/env/scRNA_env/lib/python3.10/site-packages/scvi/data/fields/_base_field.py:63: UserWarning: adata.X does not contain unnormalized count data. Are you sure this is what you want?
  self.validate_field(adata)
/ocean/projects/bio250020p/shared/opt/env/scRNA_env/lib/python3.10/site-packages/scvi/data/fields/_dataframe_field.py:187: UserWarning: Category 10 in adata.obs['_scvi_batch'] has fewer than 3 cells. Models may not train properly.
  categorical_mapping = _make_column_categorical(
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
SLURM auto-requeueing enabled. Setting signal handlers.
/ocean/projects/bio250020p/shared/opt/env/scRNA_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.
/ocean/projects/bio250020p/shared/opt/env/scRNA_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.
Training:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/60:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/60:   2%|▏         | 1/60 [02:04<2:02:03, 124.13s/it]Epoch 1/60:   2%|▏         | 1/60 [02:04<2:02:03, 124.13s/it, v_num=1, train_loss_step=766, train_loss_epoch=501]Epoch 2/60:   2%|▏         | 1/60 [02:04<2:02:03, 124.13s/it, v_num=1, train_loss_step=766, train_loss_epoch=501]Epoch 2/60:   3%|▎         | 2/60 [04:06<1:58:55, 123.02s/it, v_num=1, train_loss_step=766, train_loss_epoch=501]Epoch 2/60:   3%|▎         | 2/60 [04:06<1:58:55, 123.02s/it, v_num=1, train_loss_step=474, train_loss_epoch=461]Epoch 3/60:   3%|▎         | 2/60 [04:06<1:58:55, 123.02s/it, v_num=1, train_loss_step=474, train_loss_epoch=461]